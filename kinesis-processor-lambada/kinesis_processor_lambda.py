import json
import os
import base64
import boto3
from datetime import datetime

# Initialize the DynamoDB client outside the handler for better performance (warm starts).
# The table name is retrieved from environment variables, which will be set by SAM.
dynamodb = boto3.resource('dynamodb')
table_name = os.environ.get('DYNAMODB_TABLE_NAME')

# Ensure the table name is set, otherwise raise an error (critical for deployment).
if not table_name:
    raise ValueError("DYNAMODB_TABLE_NAME environment variable is not set.")

table = dynamodb.Table(table_name) # Reference to the DynamoDB table

def lambda_handler(event, context):
    """
    AWS Lambda function triggered by a Kinesis Data Stream.
    It processes records from the stream and stores them in a DynamoDB table.
    """
    
    # Log the entire incoming event for debugging purposes.
    # This event contains Kinesis records.
    print(f"Received event: {json.dumps(event)}")

    records_processed = 0
    records_failed = 0

    # Iterate over each record received from the Kinesis stream.
    # The 'event' object from Kinesis contains a list of 'Records'.
    for record in event['Records']:
        try:
            # Kinesis data is Base64 encoded, so it needs to be decoded first.
            # Then, it's a JSON string, which needs to be parsed into a Python dictionary.
            payload = base64.b64decode(record['kinesis']['data']).decode('utf-8')
            data = json.loads(payload)

            # Extract relevant data points from the Kinesis record.
            # Ensure these keys match the data generated by data_generator.py.
            link_id = data.get('linkId')
            timestamp = data.get('timestamp') # Use the timestamp from the data itself
            speed_mbps = data.get('speedMbps')
            latency_ms = data.get('latencyMs')
            packet_loss_rate = data.get('packetLossRate')
            region = data.get('region', 'unknown') # Default to 'unknown' if not present

            # Construct the item to be stored in DynamoDB.
            # DynamoDB requires a partition key. Using linkId#timestamp as a composite key
            # or just linkId as partition key and timestamp as sort key for time-series data.
            # For simplicity, using linkId as partition key and timestamp as sort key.
            item = {
                'LinkId': link_id, # Partition Key
                'Timestamp': timestamp, # Sort Key (for time-series queries on a given LinkId)
                'SpeedMbps': speed_mbps,
                'LatencyMs': latency_ms,
                'PacketLossRate': packet_loss_rate,
                'Region': region
            }

            # Put the item into the DynamoDB table.
            # This operation saves the processed data.
            table.put_item(Item=item)
            records_processed += 1
            print(f"Successfully stored record for LinkId: {link_id} at Timestamp: {timestamp}")

        except json.JSONDecodeError as e:
            print(f"Error decoding JSON payload: {e} - Record: {record['kinesis']['data']}")
            records_failed += 1
        except KeyError as e:
            print(f"Missing key in Kinesis record payload: {e} - Record: {payload}")
            records_failed += 1
        except Exception as e:
            print(f"An unexpected error occurred while processing record: {e} - Record: {payload}")
            records_failed += 1

    # Return a summary of the processing.
    return {
        'statusCode': 200,
        'body': json.dumps({
            'message': f'Processed {records_processed} records, failed {records_failed} records.',
            'processedCount': records_processed,
            'failedCount': records_failed
        })
    }

